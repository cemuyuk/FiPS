hydra:
  run:
    dir: ./artefacts/hydra_outputs

training: 
  num_epochs: 20
  use_cuda: True
  task: ImageClassification
  seed: 7
  validation_interval: 2
  hooks:
    in_place: True
    single: False
    embedding_key: embed_tokens
    fqn_prefix: layers
    num_batches: 10
    batch_train: True
    optimal_steps: 800

model:
  name: meta-llama/Llama-3.1-8B
  save_name: Llama_3_1_8B
  pretrained: True
  path: ./training/models/llama-3-1-8b.pth
  checkpoint_path: ./
  kwargs:
    p_budget: 0.50
    sparsity: 0.75
    structured_sparsity: False
    emb_fqn: patch_embed
    block_fqn_prefix: layers
    layer_fqn_suffixes: [up_proj, down_proj, gate_proj]
    normalization:
      in_place: False
      fqn: None
      apply_before_idx: None
    block_group_indices:
      block_group_1: [0, 1, 2, 3]
      block_group_2: [4, 5, 6, 7]
      block_group_3: [8, 9, 10, 11]
      block_group_4: [12, 13, 14, 15]
      block_group_5: [16, 17, 18, 19]
      block_group_6: [20, 21, 22, 23]
      block_group_7: [24, 25, 26, 27]
      block_group_8: [28, 29, 30, 31]
    concat_axis: 1
    mask: True
    base_requires_grad: True

# optimizer:
#   name: AdamW
#   label_smoothing: 0.11
#   kwargs:
#     lr: 3.125e-5 # effectively this is 1e-4
#     weight_decay: 0.1
#     betas: [0.9, 0.999]
#     eps: 9.999e-9 # effectively this is 1e-8
#     amsgrad: False
#   scheduler:
#     name: CosineAnnealingLR
#     kwargs: 
#       T_max: 400 # being overriden in main.py
#       eta_min: 1.0e-5  # effectively this is 1e-6

optimizer:
  name: SGD
  label_smoothing: 0.11
  kwargs:
    lr: 3.125e-5 # effectively this is 1e-4
    momentum: 0.5
    weight_decay: 0.1
  scheduler:
    name: CosineAnnealingLR
    kwargs: 
      T_max: 400 # being overriden in main.py
      eta_min: 1.0e-5  # effectively this is 1e-6

sparsifier:
  name: gradual
  sparsity: 0.75
  global_pruning: True
  grown_weights_init: 0
  pruning_ratio: 0.1
  t_accel: 80
  initial_sparsity: 0.05
  final_sparsity: 0.75
  accelerated_sparsity: 0.50
  scheduler:
    delta_t: 50
    t_end_coeff: 0.75

# TODO - check clip grad norm
data:
  name: redpajama
  batch_size: 1024
  num_workers: 2

wandb:
  ENABLE: True # Global switch for turning all wandb calls into no-ops, see ./src/package/utils/wandb_utils.py
  run_id: null # Keep null for fresh runs, otherwise, set to run_id to load ckpt
  name: ${model.name}-${sparsifier.name}-${optimizer.kwargs.lr}-pb50-local-arc
  project: SparseDecompositions2Share
  entity: cem1
  start_method: thread
  log_images: False # If True, log images when relevant to ML task

paths:
  data: 
    directory: ./data/ImageNet/ # ${oc.env:SLURM_TMPDIR}
    train: train
    val: val
  artefacts: ./artefacts/
  logs: ${paths.artefacts}/logs/
  models: ${paths.artefacts}/models/
  
benchmarking:
  latency: False
  inference_profiling: True
  training_profiling: False
  param_budget: 0.90
  compressed: True
  compiled: False
